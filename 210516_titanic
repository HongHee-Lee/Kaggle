
# -*- coding: utf-8 -*-
"""
Created on Thu Mar 19 11:59:36 2020

@author: greenysky
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import GridSearchCV

get_ipython().run_line_magic('matplotlib', 'inline')

def concat_df(train_data, test_data):
    # Returns a concatenated df of training and test set
    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)

def divide_df(all_data):
    # Returns divided dfs of training and test set
    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)


#titanic_df = pd.read_csv('c://users//green//titanic_train.csv')  #기존

df_train = pd.read_csv('c://data//train.csv')
df_test = pd.read_csv('c://data//test.csv')


df_all = concat_df(df_train, df_test)
#df_train, df_test=divide_df(df_all)




print(df_all.columns)

#Cabin 과 Survived의 상관관계
df_all['Cabin'] = df_all['Cabin'].str[:1]
print(df_all['Cabin'].head(3))

y_position=1.02
f, ax = plt.subplots(1, 1, figsize=(8, 8))
sns.countplot('Cabin', hue='Survived', data=df_all, order = df_all['Cabin'].value_counts().index)
plt.show()



#Embarked 와 Survived의 상관관계
y_position=1.02
f, ax = plt.subplots(1, 1, figsize=(8, 8))
sns.countplot('Embarked', hue='Survived', data=df_all)
plt.show()



#FamilySize 와 Survived의 상관관계
df_all['FamilySize'] = df_all['SibSp'] + df_all['Parch'] + 1 # 자신을 포함해야하니 1을 더합니다
y_position=1.02
f, ax = plt.subplots(1, 1, figsize=(8, 8))
sns.countplot('FamilySize', hue='Survived', data=df_all)
plt.show()
#df_all.drop(['FamilySize'], axis=1, inplace=True) #FamilySize 칼럼을 다시 삭제



#Pclass 와 Survived의 상관관계
y_position=1.02
f, ax = plt.subplots(1, 1, figsize=(8, 8))
sns.countplot('Pclass', hue='Survived', data=df_all)
plt.show()



#Sex 와 Survived의 상관관계
y_position=1.02
f, ax = plt.subplots(1, 1, figsize=(8, 8))
sns.countplot('Sex', hue='Survived', data=df_all)
plt.show()



#Age 와 Survived의 상관관계
#Age 데이터 결측값의 전처리를 다 하고 난후 나이와 생존률의 관계를 그려볼수있다.(하단 참고)




#데이터 확인

pd.set_option('display.max_columns',15)#모든 데이터를 펼쳐서 보여줍니다.


print('\n ### train 데이터 정보 ###  \n')
print("-"*10)
print(df_all)
print("-"*10)
print(df_all.columns)
print("-"*10)
print(df_all.info())
print("-"*10)
print('df_all columns with null values:\n', df_all.isnull().sum())
print("-"*10)
print('train columns with null values:\n', df_train.isnull().sum())
print("-"*10)

df_all['Age'].isnull().sum() #263
df_all['Cabin'].isnull().sum() #1014
df_all['Embarked'].isnull().sum() #2
df_all['Fare'].isnull().sum() #1



#Embarked 널값처리

print(df_all.loc[df_all['Embarked'].isnull()]) #확인
df_all['Embarked'].fillna('S',inplace=True) #S로 채우기
df_all['Embarked'].isnull().sum() #0



#Cabin 널값처리
df_all.sample(50) #다른 데이터와의 관계 확인
df_all['Cabin'].fillna('N',inplace=True)
df_all['Cabin'].isnull().sum() #0

##Cabin 데이터 추가 처리
#Cabin 의 ABCDEFG까지만 추출한다.
df_all['Cabin'] = df_all['Cabin'].str[:1]
print(df_all['Cabin'].head(3))

print(df_all['Cabin'].value_counts()) #유일 한 T선실은 상부 갑판에 위치하여 1등실 승객용이다. 따로 처리하지 않는다.



#Fare 널값 처리
df_all.loc[df_all.Fare.isnull()] #Fare 널값을 가진 1인은 혼자 탑승한 3등실 승객임을 확인

df=df_all[df_all['FamilySize']==1] #혼자탄 사람 데이터만 추출
print(df[['Pclass','Fare']].groupby('Pclass').mean()) # 3등실 승객중 혼자 탑승한 사람 요금 평균은 9.096707

df_all.loc[df_all.Fare.isnull(), 'Fare'] = 9.096707#Fare 널값 가진 1인에게 3등실 요금 평균 입력
df_all['Fare'] = df_all['Fare'].map(lambda i: np.log(i) if i > 0 else 0)#요금 데이터를 로그함수로 수치 변환






#Age 널값 처리
df_all['Initial']= df_all.Name.str.extract('([A-Za-z]+)\.') #lets extract the Salutations
df_all.groupby('Initial').mean()
df_all[['Initial','Age']].groupby('Initial').mean()
print(pd.crosstab(df_all['Initial'], df_all['Sex'])) # 경칭 별, 성별 별 데이터수 확인

#경칭 별로 Age 널값 일일이 확인후, 경칭 별 평균 Age를 대입 
df_all[df_all['Initial']=='Capt'].sample() #70male
df_all[df_all['Initial']=='Col'].sample(4) #47 ~ 60 male
df_all[df_all['Initial']=='Countess'].sample() #33female
df_all[df_all['Initial']=='Don'].sample() #40male
df_all[df_all['Initial']=='Dona'].sample() #39female
df_all[df_all['Initial']=='Dr'].sample(8) #1female  / 7male(age data missing: male 1)
df_all.loc[(df_all.Age.isnull())&(df_all['Initial']=='Dr'),'Age'] = df_all[df_all['Initial']=='Dr']['Age'].mean()#43.57142857142857
df_all[df_all['Initial']=='Jonkheer'].sample() #38male
df_all[df_all['Initial']=='Lady'].sample() #48female
df_all[df_all['Initial']=='Major'].sample(2) #45male, 52male
df_all[df_all['Initial']=='Master'].sample(60) #0~14.5male
df_all.loc[(df_all.Age.isnull())&(df_all['Initial']=='Master'),'Age'] = df_all[df_all['Initial']=='Master']['Age'].mean()#5.482641509433961
df_all[df_all['Initial']=='Miss'].sample(60) #female
df_all.loc[(df_all.Age.isnull())&(df_all['Initial']=='Miss'),'Age'] = df_all[df_all['Initial']=='Miss']['Age'].mean()#21.774238095238083
df_all[df_all['Initial']=='Mlle'].sample(2) #24,24female (Mlle=Mademoiselle, an unmarried woman)
df_all[df_all['Initial']=='Mme'].sample() #24female (Mme=Madame, a married woman)
df_all[df_all['Initial']=='Mr'].sample(60) #11세 이상 남성
df_all.loc[(df_all.Age.isnull())&(df_all['Initial']=='Mr'),'Age'] = df_all[df_all['Initial']=='Mr']['Age'].mean()#32.25215146299492
df_all[df_all['Initial']=='Mrs'].sample(60) #14세 이상 여성
df_all.loc[(df_all.Age.isnull())&(df_all['Initial']=='Mrs'),'Age'] = df_all[df_all['Initial']=='Mrs']['Age'].mean()#36.99411764705882
df_all[df_all['Initial']=='Ms'].sample(2) #28female, ??female
df_all.loc[(df_all.Age.isnull())&(df_all['Initial']=='Ms'),'Age'] = df_all[df_all['Initial']=='Ms']['Age'].mean()#28
df_all[df_all['Initial']=='Rev'].sample(8) #27 ~ 57 male
df_all[df_all['Initial']=='Sir'].sample() #49 male







#복잡한 경칭 정리 Dr/Military/Noble/Clergy, Master  , Miss,  Mr ,Mrs ,Ms   
df_all['Initial'].replace(['Mlle','Mme','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],
                        ['Miss','Mrs','Dr/Military/Noble/Clergy','Dr/Military/Noble/Clergy','Mrs','Dr/Military/Noble/Clergy','Dr/Military/Noble/Clergy','Dr/Military/Noble/Clergy','Dr/Military/Noble/Clergy','Dr/Military/Noble/Clergy','Dr/Military/Noble/Clergy','Dr/Military/Noble/Clergy','Ms'],inplace=True)
df_all.groupby('Initial').mean()




#경칭별 생존률 시각화
df_all.groupby('Initial')['Survived'].mean().plot.bar(fontsize=25)



"""
#널값 중간확인
print('Data columns with null values:\n', df_all.isnull().sum())
print("-"*10)
df_train, df_test=divide_df(df_all)
print('Data columns with null values:\n', df_train.isnull().sum())
print("-"*10)
"""

"""
#Age 데이터를 0~7 숫자로 카테고리화
def category_age(x):
    if x < 10:
        return 0
    elif x < 20:
        return 1
    elif x < 30:
        return 2
    elif x < 40:
        return 3
    elif x < 50:
        return 4
    elif x < 60:
        return 5
    elif x < 70:
        return 6
    elif x < 80:
        return 7
    else:
        return 8  
    
df_all['Age_cat'] = df_all['Age'].apply(category_age)

#df_all.drop(['Age'], axis=1, inplace=True)

print(df_all.sample(20))






##Age와 생존률의 관계 그래프 그리기
# 막대그래프의 크기 figure를 더 크게 설정 
plt.figure(figsize=(15,6))
#X축의 값을 순차적으로 표시하기 위한 설정 
group_names = [0, 1, 2, 3, 4, 5, 6, 7, 8]
# lambda 식에 위에서 생성한 get_category( ) 함수를 반환값으로 지정. 
# get_category(X)는 입력값으로 'Age' 컬럼값을 받아서 해당하는 cat 반환
df_all['Age_cat'] = df_all['Age'].apply(lambda x : category_age(x))
sns.barplot(x='Age_cat', y = 'Survived', hue='Sex', data=df_all, order=group_names)
df_all.drop('Age_cat', axis=1, inplace=True)
#lady, child first라는 원칙을 다시 한번 데이터를 통해서 볼수 있었다. 여자 아이들 생존률이 높다
"""






#라벨인코더 함수를 통해 문자데이터 숫자로 단순화하기
#from sklearn import preprocessing
#from sklearn.preprocessing import LabelEncoder


def encode_features(dataDF):
    features = ['Cabin', 'Sex', 'Embarked', 'Initial']
    for feature in features:
        le = preprocessing.LabelEncoder()  #0,1,2   #labelencoder설명 : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html
        le = le.fit(dataDF[feature])   #fitting is equal to training. Then, after it is trained, the model can be used to make predictions, usually with a .predict() method call.
        dataDF[feature] = le.transform(dataDF[feature])
        
    return dataDF

df_all = encode_features(df_all)
df_all.head()




#불필요한 컬럼 드랍
df_all.drop(['PassengerId', 'Name', 'Ticket','SibSp', 'Parch'], axis=1, inplace=True)
df_all.head()



df_train, df_test=divide_df(df_all)


#feature데이터 셋과 Label 데이터 셋 추출. 
X_df_train= df_train.drop('Survived',axis=1)
y_df_train = df_train['Survived']
X_df_test= df_test


#정확도 평가에 사용할 train 데이터 나누기
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test=train_test_split(X_df_train, y_df_train,
                                                  test_size=0.2, random_state=11)



from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
import lightgbm as lgb
from sklearn.metrics import accuracy_score

# 결정트리, Random Forest, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성
dt_clf = DecisionTreeClassifier(random_state=11)
rf_clf = RandomForestClassifier(random_state=11)
lr_clf = LogisticRegression()
xgb_clf = XGBClassifier()


# DecisionTreeClassifier 학습/예측/평가
dt_clf.fit(X_train , y_train)
dt_pred = dt_clf.predict(X_test)
print('DecisionTreeClassifier 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))

# RandomForestClassifier 학습/예측/평가
rf_clf.fit(X_train , y_train)
rf_pred = rf_clf.predict(X_test)
print('RandomForestClassifier 정확도:{0:.4f}'.format(accuracy_score(y_test, rf_pred)))

# LogisticRegression 학습/예측/평가
lr_clf.fit(X_train , y_train)
lr_pred = lr_clf.predict(X_test)
print('LogisticRegression 정확도: {0:.4f}'.format(accuracy_score(y_test, lr_pred)))

#XGBBoost 학습/예측/평가
xgb_clf.fit(X_train, y_train)
xgb_pred=xgb_clf.predict(X_test)
print('XGBBoost 정확도: {0:.4f}'.format(accuracy_score(y_test, xgb_pred)))

#LightGbm 학습/예측/평가
lgb_clf.fit(X_train, y_train)
lgb_pred=lgb_clf.predict(X_test)
print('Lightbgm 정확도: {0:.4f}'.format(accuracy_score(y_test, lgb_pred)))

#--------------------------------------------------------------------
#XGboost 
#GridSearchCV의 최적 하이퍼 파라미터 찾기
from sklearn.model_selection import GridSearchCV
#xgb_clf.get_params().keys()
parameters= {'learning_rate': [0.05,0.1,0.2], 'max_depth': [7,12,16], 
             'n_estimators': [3,25,35], 'seed': [10], 'n_estimator' : [35,40], 'n_jobs':[1,3], 'scale_pos_weight' :[3,5,9,13]}


grid_xgblf = GridSearchCV(xgb_clf , param_grid=parameters , scoring='accuracy' , cv=5)
grid_xgblf.fit(X_train , y_train)

print('GridSearchCV 최적 하이퍼 파라미터 :',grid_xgblf.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_xgblf.best_score_))
best_xgblf = grid_xgblf.best_estimator_

# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행. 
dpredictions = best_xgblf.predict(X_test)
accuracy = accuracy_score(y_test , dpredictions)
print('테스트 세트에서의 XGboost 정확도 : {0:.4f}'.format(accuracy))


#데이터 제출
submission = pd.read_csv('c://data//submission//gender_submission.csv')
submission.head()

dpredictions = best_xgblf.predict(X_df_test)
dpredictions=dpredictions.astype(int)
submission['Survived'] = dpredictions
submission.to_csv('C://data//submission/submission_xgb1.csv', index=False)



#-------------------------------------------------------------


#RandomForest 
#GridSearchCV의 최적 하이퍼 파라미터 찾기
from sklearn.model_selection import GridSearchCV
#rf_clf.get_params().keys()

parameters = {'max_depth':[7],
             'min_samples_split':[27], 'min_samples_leaf':[1,5,15]}

grid_rflf = GridSearchCV(rf_clf , param_grid=parameters , scoring='accuracy' , cv=5)
grid_rflf.fit(X_train , y_train)

print('GridSearchCV 최적 하이퍼 파라미터 :',grid_rflf.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_rflf.best_score_))
best_rflf = grid_rflf.best_estimator_

# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행. 
dpredictions = best_rflf.predict(X_test)
accuracy = accuracy_score(y_test , dpredictions)
print('테스트 세트에서의 LogisticRegression 정확도 : {0:.4f}'.format(accuracy))

#데이터 제출
submission = pd.read_csv('c://data//submission//gender_submission.csv')
submission.head()

dpredictions = best_rflf.predict(X_df_test)
dpredictions=dpredictions.astype(int)
submission['Survived'] = dpredictions
submission.to_csv('C://data//submission/submission_rf1.csv', index=False)




#-----------------------------------------------------------------



#DecisionTreeClassifie
#GridSearchCV의 최적 하이퍼 파라미터 찾기
from sklearn.model_selection import GridSearchCV
#dt_clf.get_params().keys()

parameters = {'max_depth':[3,7,20,40], 
             'min_samples_split':[2,10,27,40], 'min_samples_leaf':[1,5,15,30,50]}

grid_dtlf = GridSearchCV(dt_clf , param_grid=parameters , scoring='accuracy' , cv=5)
grid_dtlf.fit(X_train , y_train)

print('GridSearchCV 최적 하이퍼 파라미터 :',grid_dtlf.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dtlf.best_score_))
best_dtlf = grid_dtlf.best_estimator_

# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행. 
dpredictions = best_dtlf.predict(X_test)
accuracy = accuracy_score(y_test , dpredictions)
print('테스트 세트에서의 LogisticRegression 정확도 : {0:.4f}'.format(accuracy)) #0.8603

#데이터 제출
submission = pd.read_csv('c://data//submission//gender_submission.csv')
submission.head()

dpredictions = best_dtlf.predict(X_df_test)
dpredictions=dpredictions.astype(int)
submission['Survived'] = dpredictions
submission.to_csv('C://data//submission/submission_dt.csv', index=False)




#-----------------------------------------------------------


#LogisticRegression
#GridSearchCV의 최적 하이퍼 파라미터 찾기
from sklearn.model_selection import GridSearchCV
#lr_clf.get_params().keys()

parameters = {'C':[1,4,7,9,10,20,40,70],
              'max_iter':[12,25,28,50],
              'multi_class':['ovr','auto'],
              'random_state':[20,30,50],
              'solver':['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],
              'verbose':[0],
              'intercept_scaling':[3,20,40],

              'penalty':['l2'], 
              'class_weight':['balanced']}

grid_lrlf = GridSearchCV(lr_clf , param_grid=parameters , scoring='accuracy' , cv=5)
grid_lrlf.fit(X_train , y_train)

print('GridSearchCV 최적 하이퍼 파라미터 :',grid_lrlf.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_lrlf.best_score_))
best_lrlf = grid_lrlf.best_estimator_

# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행. 
dpredictions = best_lrlf.predict(X_test)
accuracy = accuracy_score(y_test , dpredictions)
print('테스트 세트에서의 LogisticRegression 정확도 : {0:.4f}'.format(accuracy)) #0.8715

#데이터 제출
submission = pd.read_csv('c://data//submission//gender_submission.csv')
submission.head()

dpredictions = best_lrlf.predict(X_df_test)
dpredictions=dpredictions.astype(int)
submission['Survived'] = dpredictions
submission.to_csv('C://data//submission/submission_lr.csv', index=False)





#--------------------------------------------------------------




#라이트 비지엠 결국 실패...
#lightgbm as lgb
#GridSearchCV의 최적 하이퍼 파라미터 찾기
import lightgbm as lgb
from sklearn.model_selection import GridSearchCV
#lr_clf.get_params().keys()

parameters = {'learning_rate':[0.01],
              'max_depth':[16],
              'boosting' : ['gbdt'],
              'objective' : ['regression'],
              'metric' : ['mse'],
              'is_traing_metric':[True],
              'num_leaves':[144],
              'feature_fraction':[0.9],
              'bagging_fraction':[0.7],
              'bagging_freq': [5],
              'seed' : [2020]}
model=lgb.train(params=parameters, df_train, 1000, df_test, verbose_eval=100, early_stopping_rounds=100)
dpredictions=model.predict(X_test)
accuracy = accuracy_score(y_test , dpredictions)
print('테스트 세트에서의 Lightbgm 정확도 : {0:.4f}'.format(accuracy)) #0.8715

grid_lrlf = GridSearchCV(lr_clf , param_grid=parameters , scoring='accuracy' , cv=5)
grid_lrlf.fit(X_train , y_train)

print('GridSearchCV 최적 하이퍼 파라미터 :',grid_lrlf.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_lrlf.best_score_))
best_lrlf = grid_lrlf.best_estimator_

# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행. 
dpredictions = best_lrlf.predict(X_test)
accuracy = accuracy_score(y_test , dpredictions)
print('테스트 세트에서의 LogisticRegression 정확도 : {0:.4f}'.format(accuracy)) #0.8715

#데이터 제출
submission = pd.read_csv('c://data//submission//gender_submission.csv')
submission.head()

dpredictions = best_lrlf.predict(X_df_test)
dpredictions=dpredictions.astype(int)
submission['Survived'] = dpredictions
submission.to_csv('C://data//submission/submission_lr.csv', index=False)

"""
lr_clf.get_params().keys()

parameters = {'C':[1,4,7,9,10,20,40,70],
              'max_iter':[12,25,28,50],
              'multi_class':['ovr','auto'],
              'random_state':[20,30,50],
              'solver':['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],
              'verbose':[0],
              'intercept_scaling':[20,100],

              'penalty':['l2'], 
              'class_weight':['balanced']}


grid_lrlf = GridSearchCV(lr_clf , param_grid=parameters , scoring='accuracy' , cv=5)
grid_lrlf.fit(X_train , y_train)

print('GridSearchCV 최적 하이퍼 파라미터 :',grid_lrlf.best_params_)
print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_lrlf.best_score_))
best_lrlf = grid_lrlf.best_estimator_

# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행. 
dpredictions = best_lrlf.predict(X_test)
accuracy = accuracy_score(y_test , dpredictions)
print('테스트 세트에서의 LogisticRegression 정확도 : {0:.4f}'.format(accuracy))
"""







